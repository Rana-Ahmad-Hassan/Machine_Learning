{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa818ced",
   "metadata": {},
   "source": [
    "Naive Bayes Algorithm\n",
    "\n",
    "Naive Bayes is a family of probabilistic machine learning algorithms based on Bayes' theorem. It's a popular algorithm for classification tasks, especially when dealing with high-dimensional data.\n",
    "\n",
    "How Naive Bayes Works:\n",
    "\n",
    "1. Bayes' Theorem: Naive Bayes is based on Bayes' theorem, which describes the probability of an event occurring given some prior knowledge.\n",
    "2. Independence Assumption: Naive Bayes assumes that features are independent of each other, which simplifies the calculation of probabilities.\n",
    "3. Calculate Probabilities: The algorithm calculates the probability of each class given the input features.\n",
    "4. Predict Class: The algorithm predicts the class with the highest probability.\n",
    "\n",
    "Types of Naive Bayes:\n",
    "\n",
    "1. Gaussian Naive Bayes: Used for continuous features, assuming a Gaussian distribution.\n",
    "2. Multinomial Naive Bayes: Used for discrete features, such as text classification.\n",
    "3. Bernoulli Naive Bayes: Used for binary features.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "1. Easy to Implement: Naive Bayes is a simple algorithm to implement.\n",
    "2. Fast Training: Naive Bayes is relatively fast to train, especially compared to more complex algorithms.\n",
    "3. Handling High-Dimensional Data: Naive Bayes can handle high-dimensional data with a large number of features.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "1. Independence Assumption: The independence assumption may not always hold true, which can affect accuracy.\n",
    "2. Sensitive to Feature Correlation: Naive Bayes can be sensitive to feature correlation, which can impact performance.\n",
    "\n",
    "Applications:\n",
    "\n",
    "1. Text Classification: Naive Bayes is often used for text classification tasks, such as spam detection and sentiment analysis.\n",
    "2. Image Classification: Naive Bayes can be used for image classification tasks, such as object recognition.\n",
    "3. Recommendation Systems: Naive Bayes can be used in recommendation systems to predict user preferences.\n",
    "\n",
    "When to Use:\n",
    "\n",
    "1. Simple Classification Tasks: Naive Bayes is suitable for simple classification tasks with a small number of features.\n",
    "2. High-Dimensional Data: Naive Bayes is suitable for high-dimensional data with a large number of features.\n",
    "3. Fast and Efficient: Naive Bayes is a good choice when speed and efficiency are important.\n",
    "\n",
    "Naive Bayes is a popular and effective algorithm for classification tasks, especially when dealing with high-dimensional data. However, it's essential to consider the independence assumption and feature correlation when applying Naive Bayes to real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a487b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1a71f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sns.load_dataset('iris')\n",
    "X = data.drop('species', axis=1)\n",
    "y = data['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48695813",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "random_state = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddada241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "print('Confusion Matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9447a4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 30.00%\n",
      "Confusion Matrix:\n",
      "[[ 0 10  0]\n",
      " [ 0  9  0]\n",
      " [ 0 11  0]]\n"
     ]
    }
   ],
   "source": [
    "model = BernoulliNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "# Bernoulli gives us the poor results because it wants the data in the binary like yes or no 0/1 and other\n",
    "# Gaussian works well with continuous data like height, weight, age etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d17b3",
   "metadata": {},
   "source": [
    "Bernoulli gives us the poor results because it wants the data in the binary like yes or no 0/1 and other"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
