{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e9eb0bb",
   "metadata": {},
   "source": [
    "Pipelines in Machine Learning\n",
    "\n",
    "In machine learning, a pipeline is a series of data processing steps that are connected in a workflow. Each step in the pipeline performs a specific task, such as data preprocessing, feature engineering, model selection, and prediction.\n",
    "\n",
    "Components of a Pipeline:\n",
    "\n",
    "1. Data Ingestion: Loading data from various sources.\n",
    "2. Data Preprocessing: Cleaning, transforming, and preparing data for modeling.\n",
    "3. Feature Engineering: Selecting and transforming relevant features.\n",
    "4. Model Selection: Choosing a suitable machine learning algorithm.\n",
    "5. Model Training: Training the model on the prepared data.\n",
    "6. Model Evaluation: Evaluating the performance of the trained model.\n",
    "7. Deployment: Deploying the model in a production-ready environment.\n",
    "\n",
    "Benefits of Pipelines:\n",
    "\n",
    "1. Reusability: Pipelines can be reused for similar tasks or datasets.\n",
    "2. Efficiency: Pipelines automate the workflow, reducing manual effort.\n",
    "3. Consistency: Pipelines ensure consistency in data processing and modeling.\n",
    "4. Scalability: Pipelines can be scaled up or down depending on the dataset size and complexity.\n",
    "5. Collaboration: Pipelines facilitate collaboration among data scientists and engineers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13404a5a",
   "metadata": {},
   "source": [
    "First without Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b30b27f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8156424581005587\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       110\n",
      "           1       0.78      0.72      0.75        69\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.81      0.80      0.80       179\n",
      "weighted avg       0.81      0.82      0.81       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load Titanic dataset\n",
    "data = sns.load_dataset(\"titanic\")\n",
    "\n",
    "# Drop columns with too many missing values or irrelevant\n",
    "data = data.drop(columns=[\"deck\", \"embark_town\", \"alive\", \"class\", \"who\", \"adult_male\"])\n",
    "\n",
    "# Handle missing values\n",
    "data[\"age\"] = data[\"age\"].fillna(data[\"age\"].median())\n",
    "data[\"embarked\"] = data[\"embarked\"].fillna(data[\"embarked\"].mode()[0])\n",
    "\n",
    "# Encode categorical variables manually (get_dummies)\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "# Features and target\n",
    "X = data.drop(columns=\"survived\")\n",
    "y = data[\"survived\"]\n",
    "\n",
    "# Split data\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train Decision Tree\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(train_X, train_Y)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(test_X)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy Score:\", accuracy_score(test_Y, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(test_Y, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bccc6b",
   "metadata": {},
   "source": [
    "Now with the Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d7b3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8156424581005587\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85       110\n",
      "           1       0.79      0.71      0.75        69\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.81      0.80      0.80       179\n",
      "weighted avg       0.81      0.82      0.81       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load Titanic dataset\n",
    "data = sns.load_dataset(\"titanic\")\n",
    "\n",
    "# Drop irrelevant columns\n",
    "data = data.drop(columns=[\"deck\", \"embark_town\", \"alive\", \"class\", \"who\", \"adult_male\"])\n",
    "\n",
    "# Features and target\n",
    "X = data.drop(columns=\"survived\")\n",
    "y = data[\"survived\"]\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_features = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "# Preprocessing for numeric features: fill missing + scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical features: fill missing + one-hot encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "# Combine preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Final pipeline with DecisionTree\n",
    "clf = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Split dataset\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train pipeline\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(test_X)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy Score:\", accuracy_score(test_Y, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(test_Y, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a262ac",
   "metadata": {},
   "source": [
    "'    Without Pipeline (manual preprocessing) 'Pros\n",
    "\n",
    "Transparent: You see each preprocessing step (fillna, encoding, scaling, etc.) explicitly.\n",
    "\n",
    "Flexible debugging: Easy to test transformations step by step.\n",
    "\n",
    "Good for learning: Helps beginners understand preprocessing clearly.\n",
    "\n",
    "Cons\n",
    "\n",
    "Repetition: You must reapply the exact same preprocessing to test data, future unseen data, or deployment manually.\n",
    "\n",
    "Error-prone: Easy to forget a step (e.g., forgetting to scale test data).\n",
    "\n",
    "Messy code: Preprocessing and model training code can get long and hard to maintain.\n",
    "\n",
    "Not reusable: If you want to try another model, you must duplicate preprocessing code.\n",
    "\n",
    "With Pipeline\n",
    "Pros\n",
    "\n",
    "Cleaner & modular: Preprocessing + model = one object (Pipeline).\n",
    "\n",
    "Consistency: Ensures the same transformations are applied to both training and testing data.\n",
    "\n",
    "Easy to deploy: You can just .fit() on train and .predict() on new data, everything is handled automatically.\n",
    "\n",
    "Works with hyperparameter tuning: You can directly tune model + preprocessing steps inside GridSearchCV / RandomizedSearchCV.\n",
    "\n",
    "Production ready: Saves you from data leakage because transformations fit only on training data inside the pipeline.\n",
    "\n",
    "Cons\n",
    "\n",
    "Less transparent: Harder to debug inside (need to check intermediate steps using .named_steps).\n",
    "\n",
    "Learning curve: Beginners may find ColumnTransformer and Pipeline syntax confusing.\n",
    "\n",
    "Less control: If you want custom preprocessing logic (like domain-specific feature engineering), it might feel restrictive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ed9d2",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "Use manual preprocessing when you’re exploring or learning step-by-step.\n",
    "\n",
    "Use a Pipeline for real-world projects, experiments with multiple models, or production — it’s more robust, reusable, and less error-prone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
